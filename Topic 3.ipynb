{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "df60 = pd.read_csv(\"MSJO 60 01-03 2023.csv\", delimiter=\";\")\n",
    "df900 = pd.read_csv(\"MSJO 900 01-03 2023.csv\", delimiter=\";\")\n",
    "\n",
    "# Set PVALUE to float\n",
    "df60[\"PVALUE\"] = df60[\"PVALUE\"].str.replace(\",\", \".\").astype(float)\n",
    "df900[\"PVALUE\"] = df900[\"PVALUE\"].str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "# Duplicate deletion\n",
    "df60.drop_duplicates(inplace=True)\n",
    "df900.drop_duplicates(inplace=True)\n",
    "\n",
    "# Delete rows with empty values\n",
    "df60 = df60.dropna(subset=[\"MESS_ID\", \"VALUEDATE\", \"PVALUE\"], how=\"any\")\n",
    "df900 = df900.dropna(subset=[\"MESS_ID\", \"VALUEDATE\", \"PVALUE\"], how=\"any\")\n",
    "\n",
    "# Convert VALUEDATE to datetime\n",
    "df60[\"VALUEDATE\"] = pd.to_datetime(df60[\"VALUEDATE\"])\n",
    "df900[\"VALUEDATE\"] = pd.to_datetime(df900[\"VALUEDATE\"])\n",
    "\n",
    "# Filter date range\n",
    "df60 = df60[(df60['VALUEDATE'] > '2023-01-15') & (df60['VALUEDATE'] < '2023-03-20')]\n",
    "df900 = df900[(df900['VALUEDATE'] > '2023-01-15') & (df900['VALUEDATE'] < '2023-03-20')]\n",
    "\n",
    "# Define a function to filter outliers for each MESS_ID\n",
    "def filter_outliers(df):\n",
    "    # Group-wise calculation of quartiles and upper thresholds\n",
    "    grouped = df.groupby(\"MESS_ID\")\n",
    "    filtered_dfs = []\n",
    "\n",
    "    for name, group in grouped:\n",
    "        # Calculate quartiles\n",
    "        Q1 = group[\"PVALUE\"].quantile(0.25)\n",
    "        Q3 = group[\"PVALUE\"].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Filter outliers\n",
    "        filtered_group = group[(group[\"PVALUE\"] >= Q1 - IQR) & (group[\"PVALUE\"] <= Q3  + IQR)]\n",
    "        \n",
    "        # Check if there are filtered rows\n",
    "        if not filtered_group.empty:\n",
    "            # Append filtered group to list\n",
    "            filtered_dfs.append(filtered_group)\n",
    "    \n",
    "    # Concatenate filtered dataframes if there are any\n",
    "    if filtered_dfs:\n",
    "        df_filtered = pd.concat(filtered_dfs)\n",
    "        return df_filtered\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Filter outliers for df60 and df900 separately\n",
    "df60_filtered = filter_outliers(df60)\n",
    "df900_filtered = filter_outliers(df900)\n",
    "\n",
    "if df60_filtered is not None:\n",
    "    df60_filtered.sort_values(by=[\"MESS_ID\", \"VALUEDATE\"], ascending=[True, True], inplace=True)\n",
    "\n",
    "if df900_filtered is not None:\n",
    "    df900_filtered.sort_values(by=[\"MESS_ID\", \"VALUEDATE\"], ascending=[True, True], inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "if df60_filtered is not None:\n",
    "    df60_filtered.to_csv(\"cleaned60.csv\", index=False)\n",
    "\n",
    "if df900_filtered is not None:\n",
    "    df900_filtered.to_csv(\"cleaned900.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaqu\\AppData\\Local\\Temp\\ipykernel_9072\\1974744881.py:21: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_60s_resampled = df60['avg_value'].resample('15T').mean()\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it is not monotonic and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARIMA 60-second resampled MSE: 445.7801972791652\n",
      "ARIMA 15-minute MSE: 310980.0456589194\n"
     ]
    }
   ],
   "source": [
    "# ARIMA Modell\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df60 = pd.read_csv(\"cleaned60.csv\")\n",
    "df900 = pd.read_csv(\"cleaned900.csv\")\n",
    "\n",
    "# Convert 'VALUEDATE' column to datetime and set it as index\n",
    "df60['VALUEDATE'] = pd.to_datetime(df60['VALUEDATE'])\n",
    "df60.set_index('VALUEDATE', inplace=True)\n",
    "\n",
    "df900['VALUEDATE'] = pd.to_datetime(df900['VALUEDATE'])\n",
    "df900.set_index('VALUEDATE', inplace=True)\n",
    "\n",
    "# Calculate the average value for each DataFrame\n",
    "df60['avg_value'] = df60.mean(axis=1)\n",
    "df900['avg_value'] = df900.mean(axis=1)\n",
    "\n",
    "# Resample 60-second data to 15-minute intervals and handle missing values\n",
    "df_60s_resampled = df60['avg_value'].resample('15T').mean()\n",
    "df_60s_resampled = df_60s_resampled.fillna(df_60s_resampled.mean())\n",
    "\n",
    "# Ensure df900's avg_value is handled similarly\n",
    "df900['avg_value'] = df900['avg_value'].fillna(df900['avg_value'].mean())\n",
    "\n",
    "# Train-Test Split\n",
    "train_size_60s = int(len(df_60s_resampled) * 0.8)\n",
    "train_size_15min = int(len(df900) * 0.8)\n",
    "\n",
    "train_60s, test_60s = df_60s_resampled[:train_size_60s], df_60s_resampled[train_size_60s:]\n",
    "train_15min, test_15min = df900['avg_value'][:train_size_15min], df900['avg_value'][train_size_15min:]\n",
    "\n",
    "# Train ARIMA model on 60-second resampled data\n",
    "arima_60s = ARIMA(train_60s, order=(5, 1, 0))\n",
    "arima_60s_fit = arima_60s.fit()\n",
    "\n",
    "# Train ARIMA model on 15-minute data\n",
    "arima_15min = ARIMA(train_15min, order=(5, 1, 0))\n",
    "arima_15min_fit = arima_15min.fit()\n",
    "\n",
    "# Forecasting\n",
    "arima_60s_forecast = arima_60s_fit.forecast(steps=len(test_60s))\n",
    "arima_15min_forecast = arima_15min_fit.forecast(steps=len(test_15min))\n",
    "\n",
    "# Evaluation\n",
    "arima_60s_mse = mean_squared_error(test_60s, arima_60s_forecast)\n",
    "arima_15min_mse = mean_squared_error(test_15min, arima_15min_forecast)\n",
    "\n",
    "print(f'ARIMA 60-second resampled MSE: {arima_60s_mse}')\n",
    "print(f'ARIMA 15-minute MSE: {arima_15min_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n",
      "C:\\Users\\joaqu\\AppData\\Local\\Temp\\ipykernel_9072\\982723542.py:20: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  df_60s_resampled = df60['avg_value'].resample('15T').mean().fillna(df60.mean(axis=1).mean())\n",
      "17:13:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "17:13:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "17:13:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prophet\\forecaster.py:1854: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  dates = pd.date_range(\n",
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prophet\\forecaster.py:1854: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  dates = pd.date_range(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet 60-second resampled MSE: 15909.63097637916\n",
      "Prophet 15-minute MSE: 85273.33433007599\n"
     ]
    }
   ],
   "source": [
    "# Prophet Modell\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "\n",
    "# Laden der 60-Sekunden-Daten\n",
    "df60 = pd.read_csv('cleaned60.csv')\n",
    "df60['VALUEDATE'] = pd.to_datetime(df60['VALUEDATE'])  # Umwandlung der 'VALUEDATE'-Spalte in Datetime\n",
    "df60.set_index('VALUEDATE', inplace=True)  # Setzen der 'VALUEDATE'-Spalte als Index\n",
    "\n",
    "# Laden der 15-Minuten-Daten\n",
    "df900 = pd.read_csv('cleaned900.csv')\n",
    "df900['VALUEDATE'] = pd.to_datetime(df900['VALUEDATE'])  # Umwandlung der 'VALUEDATE'-Spalte in Datetime\n",
    "df900.set_index('VALUEDATE', inplace=True)  # Setzen der 'VALUEDATE'-Spalte als Index\n",
    "\n",
    "# Fehlende Werte mit dem Mittelwert füllen\n",
    "df60['avg_value'] = df60.mean(axis=1).fillna(df60.mean(axis=1).mean())\n",
    "df900['avg_value'] = df900.mean(axis=1).fillna(df900.mean(axis=1).mean())\n",
    "\n",
    "# Resample 60-Sekunden-Daten auf 15-Minuten-Intervalle\n",
    "df_60s_resampled = df60['avg_value'].resample('15T').mean().fillna(df60.mean(axis=1).mean())\n",
    "\n",
    "# Vorbereitung der Daten für Prophet\n",
    "df_60s_prophet = df_60s_resampled.reset_index().rename(columns={'VALUEDATE': 'ds', 'avg_value': 'y'})\n",
    "df_15min_prophet = df900.reset_index().rename(columns={'VALUEDATE': 'ds', 'avg_value': 'y'})\n",
    "\n",
    "# Train-Test Split\n",
    "train_size = int(len(df_60s_prophet) * 0.8)\n",
    "train_60s_prophet = df_60s_prophet[:train_size]\n",
    "test_60s_prophet = df_60s_prophet[train_size:]\n",
    "\n",
    "train_15min_prophet = df_15min_prophet[:train_size]\n",
    "test_15min_prophet = df_15min_prophet[train_size:]\n",
    "\n",
    "# Training des Prophet-Modells auf 60-Sekunden-Daten\n",
    "model_60s = Prophet()\n",
    "model_60s.fit(train_60s_prophet)\n",
    "\n",
    "# Training des Prophet-Modells auf 15-Minuten-Daten\n",
    "model_15min = Prophet()\n",
    "model_15min.fit(train_15min_prophet)\n",
    "\n",
    "# Prognose\n",
    "future_60s = model_60s.make_future_dataframe(periods=len(test_60s_prophet), freq='15T')\n",
    "forecast_60s = model_60s.predict(future_60s)\n",
    "\n",
    "future_15min = model_15min.make_future_dataframe(periods=len(test_15min_prophet), freq='15T')\n",
    "forecast_15min = model_15min.predict(future_15min)\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_60s_values = test_60s_prophet['y'].values\n",
    "forecast_60s_values = forecast_60s.iloc[-len(test_60s_prophet):]['yhat'].values\n",
    "mse_60s_prophet = mean_squared_error(test_60s_values, forecast_60s_values)\n",
    "\n",
    "test_15min_values = test_15min_prophet['y'].values\n",
    "forecast_15min_values = forecast_15min.iloc[-len(test_15min_prophet):]['yhat'].values\n",
    "mse_15min_prophet = mean_squared_error(test_15min_values, forecast_15min_values)\n",
    "\n",
    "print(f'Prophet 60-second resampled MSE: {mse_60s_prophet}')\n",
    "print(f'Prophet 15-minute MSE: {mse_15min_prophet}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for energy produced every minute:\n",
      "Mean Squared Error: 394.5005984799122\n",
      "Root Mean Squared Error (RMSE): 19.862039131970114\n",
      "RMSE as a percentage of the mean: 377.01568757946455 %\n",
      "\n",
      "Results for energy produced every 15 minutes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 57570.31463116803\n",
      "Root Mean Squared Error (RMSE): 239.93814751132848\n",
      "RMSE as a percentage of the mean: 808.1374311510068 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaqu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def preprocess_data(data, is_energy_data=False):\n",
    "    if is_energy_data:\n",
    "        # If 'VALUEDATE' exists, preprocess it\n",
    "        if 'VALUEDATE' in data.columns:\n",
    "            # Convert 'VALUEDATE' to datetime\n",
    "            data['VALUEDATE'] = pd.to_datetime(data['VALUEDATE'])\n",
    "            # Extract year, month, day, hour, and minute\n",
    "            data['Year'] = data['VALUEDATE'].dt.year\n",
    "            data['Month'] = data['VALUEDATE'].dt.month\n",
    "            data['Day'] = data['VALUEDATE'].dt.day\n",
    "            data['Hour'] = data['VALUEDATE'].dt.hour\n",
    "            data['Minute'] = data['VALUEDATE'].dt.minute\n",
    "            # Drop the original 'VALUEDATE' column\n",
    "            data.drop(columns=['VALUEDATE'], inplace=True)\n",
    "    return data\n",
    "\n",
    "def train_and_evaluate(energy_data_file, components_data_file, energy_id_col, components_id_col):\n",
    "    # Load the energy data\n",
    "    energy_data = pd.read_csv(energy_data_file)\n",
    "    # Preprocess the energy data\n",
    "    energy_data = preprocess_data(energy_data, is_energy_data=True)\n",
    "\n",
    "    # Load the components data\n",
    "    components_data = pd.read_csv(components_data_file, sep=';')\n",
    "\n",
    "    # Merge energy data with components data on 'MESS_ID'\n",
    "    merged_data = pd.merge(energy_data, components_data, left_on=energy_id_col, right_on=components_id_col)\n",
    "\n",
    "    # Assuming you want to predict energy production based on the component ID and time features\n",
    "    X = merged_data[['CNTL_ID', 'Year', 'Month', 'Day', 'Hour', 'Minute']]  # Independent variables\n",
    "    y = merged_data['PVALUE']                                                # Dependent variable\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the testing set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# File paths for the two datasets\n",
    "energy_data_minute = 'cleaned60.csv'\n",
    "energy_data_15_minutes = 'cleaned900.csv'\n",
    "components_data_file = 'TB Counter.csv'  # Considering ';' as separator\n",
    "\n",
    "# Column names to use for merging\n",
    "energy_id_col = 'MESS_ID'\n",
    "components_id_col = 'CNTL_ID'\n",
    "\n",
    "print(\"Results for energy produced every minute:\")\n",
    "train_and_evaluate(energy_data_minute, components_data_file, energy_id_col, components_id_col)\n",
    "\n",
    "print(\"\\nResults for energy produced every 15 minutes:\")\n",
    "train_and_evaluate(energy_data_15_minutes, components_data_file, energy_id_col, components_id_col)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
